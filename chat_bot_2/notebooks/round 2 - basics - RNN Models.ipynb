{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "- Text classification and generation using RNNs\n",
    "- Types/modes of RNNs\n",
    "- Using IMDB reviews dataset \n",
    "- [Ref: Tensor Guide](https://www.tensorflow.org/tutorials/text/text_classification_rnn)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<a href=\"javascript:code_toggle()\">Show/Hide Code</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "import os, random, pickle \n",
    "\n",
    "from termcolor import colored\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data - TFDS \n",
    "TFDS = Public datasets in an easy to use format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# a collection of ready to use datasets of type tf.data.Datasets\n",
    "import tensorflow_datasets as tfds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Helper functions\n",
    "## Plot graphs \n",
    "def plot_graphs(fhist, metric): \n",
    "    plt.plot(fhist[ metric ] )\n",
    "    plt.plot( fhist['val_'+metric ] , '')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel( metric )\n",
    "    plt.legend( [ metric, 'val_'+metric ] )\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Download IMDB dataset using TFDS\n",
    "dataset, infor = tfds.load(\"imdb_reviews/subwords8k\", with_info=True, as_supervised=True)\n",
    "\n",
    "train_x, test_x = dataset['train'], dataset['test']\n",
    "\n",
    "# info has the text encoder, in this case tfds.features.text.SubwordTextEncoder \n",
    "encoder = infor.features['text'].encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Infor: tfds.core.DatasetInfo(\n",
      "    name='imdb_reviews',\n",
      "    version=1.0.0,\n",
      "    description='Large Movie Review Dataset.\n",
      "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.',\n",
      "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
      "    features=FeaturesDict({\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
      "        'text': Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=8185>),\n",
      "    }),\n",
      "    total_num_examples=100000,\n",
      "    splits={\n",
      "        'test': 25000,\n",
      "        'train': 25000,\n",
      "        'unsupervised': 50000,\n",
      "    },\n",
      "    supervised_keys=('text', 'label'),\n",
      "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
      "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
      "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
      "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
      "      month     = {June},\n",
      "      year      = {2011},\n",
      "      address   = {Portland, Oregon, USA},\n",
      "      publisher = {Association for Computational Linguistics},\n",
      "      pages     = {142--150},\n",
      "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
      "    }\"\"\",\n",
      "    redistribution_info=,\n",
      ")\n",
      "\n",
      "Number of training observations: 25000\n",
      "Number of testing observations: 25000\n",
      "Number of unique labels/classes: 2\n",
      "\tClass Labels: ['neg', 'pos']\n",
      "\n",
      "Encoder: <SubwordTextEncoder vocab_size=8185>\n",
      "Encoder.Vocab_size: 8185\n",
      "Encoder.example: The quick brown fox ==> [19, 5161, 5798, 102, 1169, 8049]\n",
      "\t19 -->The \t5161 -->quick \t5798 -->brow\t102 -->n \t1169 -->fo\t8049 -->x\n",
      "Sample text: <generator object _eager_dataset_iterator at 0x0000021E35E0F248>\n"
     ]
    }
   ],
   "source": [
    "print(\"{} {}\".format( colored(\"Dataset Infor:\", \"blue\"), infor) ) \n",
    "      \n",
    "      \n",
    "print(\"Number of training observations: {}\".format( infor.splits['train'].num_examples  ) )\n",
    "print(\"Number of testing observations: {}\".format( infor.splits['test'].num_examples ) )\n",
    "\n",
    "print(\"Number of unique labels/classes: {}\\n\\tClass Labels: {}\".format( \n",
    "    infor.features['label'].num_classes, infor.features['label'].names ) )\n",
    "\n",
    "print(\"\\nEncoder: {}\".format(encoder) ) \n",
    "print(\"Encoder.Vocab_size: {}\".format(encoder.vocab_size ) ) \n",
    "s = \"The quick brown fox\" \n",
    "es = encoder.encode(s)\n",
    "print(\"Encoder.example: {} ==> {}\".format(s, es ) ) \n",
    "for i in es:\n",
    "    e = encoder.decode([i])\n",
    "    print(\"\\t{} -->{}\".format(i, e ), end=\"\" ) \n",
    "\n",
    "print(\"\\nSample text: {}\".format( tfds.as_numpy(train_x) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### \n",
    "# 1. Batch datasets and zero pad sequence lengths to longest len or max size \n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_x = (train_x.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE, padded_shapes=([None], [])))\n",
    "test_x = (test_x.padded_batch(BATCH_SIZE, padded_shapes=([None], [])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build Model\n",
    "LSTM RNN\n",
    "- Model Arch = \\[ embed, LSTM RNN, Dense, Output(1, p(x)) \\]\n",
    "- Wrap the LSTM with Bidirectional layer so as to learn long range dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Model Arch = [ embed, LSTM RNN, Dense, Output(1, p(x)) ]\n",
    "# Wrap the LSTM with Bidirectional layer so as to learn long range dependencies \n",
    "\n",
    "def get_lstm_model(emb_vocab_size, unitz=64, wrap_bidirection=True, n_lstmz=1):\n",
    "        \n",
    "    md = tf.keras.Sequential()\n",
    "    # embedding \n",
    "    md.add( tf.keras.layers.Embedding( emb_vocab_size, unitz) )\n",
    "    # n lstm layers \n",
    "    if n_lstmz == 1:\n",
    "        lstm = tf.keras.layers.LSTM(unitz)\n",
    "        md.add( tf.keras.layers.Bidirectional(lstm) if wrap_bidirection else lstm )\n",
    "    else:\n",
    "        f_l = 1\n",
    "        # full unitz on first n-1 and return sequence << TODO: the matrix dimensions math and see how to change unitz\n",
    "        lstm = tf.keras.layers.LSTM(unitz, return_sequences=True)\n",
    "        for _ in range(n_lstmz-f_l):  ## less first and last layers\n",
    "            md.add( tf.keras.layers.Bidirectional(lstm) if wrap_bidirection else lstm )\n",
    "        \n",
    "        # half unitz on last \n",
    "        md.add( tf.keras.layers.Bidirectional(lstm) if wrap_bidirection else lstm )\n",
    "        \n",
    "    # outputs @ relu, dropout=0.5 and p(x)\n",
    "    md.add(tf.keras.layers.Dense(unitz, activation='relu') )\n",
    "    if n_lstmz > 1:\n",
    "        md.add( tf.keras.layers.Dropout(rate=0.5) ) #TODO: move out \n",
    "    md.add( tf.keras.layers.Dense( 1 ) )\n",
    "\n",
    "    return md\n",
    "\n",
    "def compile_2class_lstm_model(emb_vocab_size, unitz=64, wrap_bidirection=True, n_lstmz=1):\n",
    "    md = get_lstm_model(emb_vocab_size, unitz, wrap_bidirection, n_lstmz)\n",
    "    \n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "    metrics = ['accuracy']\n",
    "    md.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "    return md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 64)          523840    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               66048     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 598,209\n",
      "Trainable params: 598,209\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = compile_2class_lstm_model( encoder.vocab_size)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "391/391 - 4372s - loss: 0.6493 - accuracy: 0.5594 - val_loss: 0.4673 - val_accuracy: 0.7819\n",
      "Epoch 2/10\n",
      "391/391 - 2471s - loss: 0.3486 - accuracy: 0.8515 - val_loss: 0.3293 - val_accuracy: 0.8505\n",
      "Epoch 3/10\n",
      "391/391 - 2552s - loss: 0.2508 - accuracy: 0.9016 - val_loss: 0.3225 - val_accuracy: 0.8725\n",
      "Epoch 4/10\n",
      "391/391 - 2632s - loss: 0.2066 - accuracy: 0.9238 - val_loss: 0.3497 - val_accuracy: 0.8737\n",
      "Epoch 5/10\n"
     ]
    }
   ],
   "source": [
    "##### A. Train \n",
    "def train_model( model, train_x, epochs=10, test_epochs=3, batch_size=64 ):\n",
    "    # 1. create callback for early stopping on validation loss if not loss decrease in two consecutive tries\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
    "    ]\n",
    "    \n",
    "    # 2. train unsupervised?? \n",
    "    train_history = model.fit(\n",
    "        train_x, \n",
    "        epochs = epochs,\n",
    "        callbacks=callbacks,\n",
    "        validation_data=test_x, \n",
    "        verbose=2, # log once per epoch = 2       \n",
    "#         batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # 3. save model\n",
    "    model.save( 'IMDB_ltsm_model.rnn') \n",
    "    \n",
    "    return train_history.history\n",
    "\n",
    "fhist = train_model(model, train_x)\n",
    "\n",
    "print(\"\\n{}\\n\".format( fhist ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### B. Validate\n",
    "def pad_to_size(vec, size):\n",
    "    zeroz = [0] * (size - len(np.array(vec) ) )\n",
    "    vec.extend(zeroz)\n",
    "    return vec\n",
    "\n",
    "test_loss, test_acc = model.evaluate( test_x )\n",
    "print(\"Without Padding: loss = {} \\taccuracy={}\".format(test_loss, test_acc ) )\n",
    "\n",
    "\n",
    "# test_loss, test_acc = model.evaluate( pad_to_size(test_x, 64 ) ) \n",
    "# print(\"With Padding: loss = {} \\taccuracy={}\".format(test_loss, test_acc ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Graphs \n",
    "plot_graphs(fhist, 'accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(fhist, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(observation, pad=False):\n",
    "    enc_observation = encoder.encode( observation )\n",
    "    \n",
    "    if pad:\n",
    "        enc_observation = pad_to_size( enc_observation, 64)\n",
    "        \n",
    "    enc_observation = tf.cast( enc_observation, tf.float32 )\n",
    "    \n",
    "    pred = model.predict( tf.expand_dims(enc_observation, 0 ) )\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_reviews = [ \"That was such a good movie!\", \"That was terribly good!\", \n",
    "                  \"That was such a bad movie!\", \"Amazing! How can something so bad be out there!\"]\n",
    "\n",
    "print(\"\\n ==== NO PADDING ==== \\n\")\n",
    "for s in sample_reviews:\n",
    "    print(\"{} ===> {}\".format(s, predict_sentiment(s) ) ) \n",
    "    \n",
    "# print(\"\\n ==== YES PADDING ==== \\n\")\n",
    "# for s in sample_reviews:\n",
    "#     print(\"{} ===> {}\".format(s, predict_sentiment(s, pad=True) ) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Stack multiple LSTM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_lsmz = 3\n",
    "\n",
    "## create model\n",
    "model = compile_2class_lstm_model( encoder.vocab_size, n_lstmz=n_lsmz)\n",
    "model.summary()\n",
    "\n",
    "## train model \n",
    "fhist = train_model(model, train_x)\n",
    "print(\"\\n{}\\n\".format( fhist ) )\n",
    "\n",
    "## evaluate\n",
    "test_loss, test_acc = model.evaluate( test_x )\n",
    "print(\"\\nWithout Padding: loss = {} \\taccuracy={}\".format(test_loss, test_acc ) )\n",
    "\n",
    "# test_loss, test_acc = model.evaluate( pad_to_size(test_x , 64) ) \n",
    "# print(\"\\nWith Padding: loss = {} \\taccuracy={}\".format(test_loss, test_acc ) )\n",
    "\n",
    "## predict\n",
    "print(\"\\n ==== NO PADDING ==== \\n\")\n",
    "for s in sample_reviews:\n",
    "    print(\"{} ===> {}\".format(s, predict_sentiment(s) ) ) \n",
    "    \n",
    "# print(\"\\n ==== YES PADDING ==== \\n\")\n",
    "# for s in sample_reviews:\n",
    "#     print(\"{} ===> {}\".format(s, predict_sentiment(s, pad=True) ) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Graphs \n",
    "plot_graphs(fhist, 'accuracy')\n",
    "plot_graphs(fhist, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
